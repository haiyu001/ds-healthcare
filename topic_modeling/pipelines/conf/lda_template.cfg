[LDA]
    domain = webmd
    drop_non_english = True
    num_partitions = 24

[Corpus]
    corpus_word_pos_candidates = NOUN,PROPN,ADJ,ADV,VERB
    corpus_vocab_size = 10000

[Train]
    iterations = 1500
    alpha_list = [1.25]
    optimize_interval_list = [40]
    num_topics_list = [105, 110, 115, 120, 125, 130, 135, 140]
    lda_vis = True

[Finetuning]
    target_alpha = 1.25
    target_optimize_interval = 40
    target_num_topics = 130
    merge_random_state = 1
    merge_threshold = 0.05
    run_merge_visualization = True
    grouping_threshold = 0.35
    run_merge_grouping = True

[Inference]
    # if the corpus size is big then increase num partitions
    inference_docs_num_partitions = 10
    # inference_threshold could be 0.075, 0.1
    inference_threshold = 0.075
    text_docs_num_partitions = 10

[Default]
    topic_modeling_folder = topic_modeling
    corpus_folder = lda_corpus
    extraction_folder = extraction
    filter_unigram_filename = filter_unigram.csv
    filter_phrase_filename = filter_phrase.csv
    corpus_word_to_lemma_filename = corpus_word_to_lemma.json

    train_docs_folder_name = train_docs
    train_corpus_file_name = train_corpus
